# RAG Chatbot: Retrieval-Augmented Generation Chatbot

A simple and modular Retrieval-Augmented Generation (RAG) chatbot built using **LangChain**, **OpenAI**, **ChromaDB**, and **Gradio**. This project demonstrates how to combine a local document knowledge base with an LLM to build a context-aware question-answering system.

---

## Features

- âœ… Upload and embed your own documents
- ğŸ” Retrieves relevant chunks based on user query
- ğŸ’¬ Answers questions using OpenAI (GPT-3.5+)
- ğŸ›¢ Vector database powered by Chroma
- âš™ï¸ Modular design using LangChain
- ğŸ¨ Simple Gradio front end

---

## Project Structure
```
RAG-chatbot/
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ ingest.py # Load and index documents into ChromaDB
â”‚ â”œâ”€â”€ chatbot.py # Set up retriever + LLM pipeline (LangChain RetrievalQA)
â”‚ â”œâ”€â”€ gradio_app.py # Gradio web interface
â”‚ â””â”€â”€ data/
â”‚ â””â”€â”€ example.txt # Sample source document
â”œâ”€â”€ db/ # Persisted ChromaDB vectorstore
â”œâ”€â”€ .env # (Optional) API keys
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## Getting Started

### 1. Clone the Repo

```bash
git clone https://github.com/yourusername/RAG-chatbot.git
cd RAG-chatbot

conda create -n rag-chatbot python=3.10
conda activate rag-chatbot

pip install -r requirements.txt
```
### 2. Add OpenAI Key
```env
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
```

### 3. Ingest Documents
```bash
python app/ingest.py
```

### 4. Run the App
```bash
python app/gradio_app.py
```
Go to http://localhost:7860 to interact with the chatbot